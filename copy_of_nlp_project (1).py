# -*- coding: utf-8 -*-
"""Copy of NLP_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TZxvYgGMJO-SbcM3N-0Eq6FCnx-e7UJK
"""

from google.colab import drive
drive.mount('/content/drive')

ar_en_path = "/content/drive/MyDrive/marian (1)/finetuned_marian_ar_en"
en_ar_path = "/content/drive/MyDrive/marian (1)/finetuned_marian_en_ar"

# Step 3: Fine-tune Arabic → English Model
from transformers import MarianTokenizer, MarianMTModel

model_checkpoint = "Helsinki-NLP/opus-mt-ar-en"
tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)
model = MarianMTModel.from_pretrained(model_checkpoint)
max_length = 128

def preprocess_function(examples):
    src_texts = [ex["ar"] for ex in examples["translation"]]
    tgt_texts = [ex["en"] for ex in examples["translation"]]
    return tokenizer(src_texts, text_target=tgt_texts, max_length=max_length, truncation=True)

tokenized_datasets = split_datasets.map(preprocess_function, batched=True)

from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

training_args = Seq2SeqTrainingArguments(
    output_dir="/content/marian_ar_en_results",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    num_train_epochs=3,
    save_total_limit=2,
    predict_with_generate=True,
    logging_dir="/content/logs_ar_en",
    logging_steps=50,
    report_to="none",
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

# Save fine-tuned Arabic → English model to Google Drive
model.save_pretrained(ar_en_path)
tokenizer.save_pretrained(ar_en_path)

import torch
from transformers import MarianTokenizer, MarianMTModel
ar_en_path="/content/drive/MyDrive/marian (1)/finetuned_marian_ar_en"
ar_en_tokenizer = MarianTokenizer.from_pretrained(ar_en_path)
ar_en_model = MarianMTModel.from_pretrained(ar_en_path).to("cuda" if torch.cuda.is_available() else "cpu")

# Step 6: Translate sample Arabic sentences (test block)
def translate_arabic_to_english(arabic_sentence, model, tokenizer, max_length=128):
    inputs = tokenizer(arabic_sentence, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)
    output_ids = model.generate(**inputs, max_length=max_length, num_beams=4)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Arabic sentences to test
examples = [
    "كيف حالك؟",
    "أنا أحب تعلم اللغة الإنجليزية.",
    "هل يمكنك مساعدتي؟",
    "أين أقرب محطة قطار؟",
    "الطقس جميل اليوم.",
    "ما اسمك؟",
    "من فضلك، انتظر هنا.",
    "أريد كوباً من الماء.",
    "شكراً جزيلاً لك!",
    "هذا الكتاب جديد.",
    "اريد الذهاب الي النوم بعد هذا اليوم الطويل",
    "لقد ارهقني هذا المشروع فعلا"
]

# Run translations
for arabic in examples:
    english = translate_arabic_to_english(arabic, ar_en_model, ar_en_tokenizer)
    print(f"Arabic: {arabic}")
    print(f"English: {english}")
    print("-" * 30)

from transformers import MarianTokenizer, MarianMTModel, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments

# Load the checkpoint
model_checkpoint = "Helsinki-NLP/opus-mt-en-ar"
tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)
model = MarianMTModel.from_pretrained(model_checkpoint)

# Tokenization function for English → Arabic
max_length = 128
def preprocess_function(examples):
    src_texts = [ex["en"] for ex in examples["translation"]]
    tgt_texts = [ex["ar"] for ex in examples["translation"]]
    return tokenizer(src_texts, text_target=tgt_texts, max_length=max_length, truncation=True)

# Reuse the already split dataset from earlier
tokenized_datasets = split_datasets.map(preprocess_function, batched=True)

# Define training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/marian_en_ar_results",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    num_train_epochs=3,
    save_total_limit=2,
    predict_with_generate=True,
    logging_dir="/content/logs_en_ar",
    logging_steps=50,
    report_to="none",
)

# Define data collator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

# Save fine-tuned English → Arabic model to Google Drive
model.save_pretrained(en_ar_path)
tokenizer.save_pretrained(en_ar_path)

from transformers import MarianMTModel, MarianTokenizer
import torch

en_ar_path = "/content/drive/MyDrive/marian (1)/finetuned_marian_en_ar"
en_ar_tokenizer = MarianTokenizer.from_pretrained(en_ar_path)
en_ar_model = MarianMTModel.from_pretrained(en_ar_path).to("cuda" if torch.cuda.is_available() else "cpu")

def translate_english_to_arabic(english_sentence, model, tokenizer, max_length=128):
    inputs = tokenizer(english_sentence, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)
    output_ids = model.generate(**inputs, max_length=max_length, num_beams=4)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# English sentences to test
examples = [
    "How are you?",
    "I love learning Arabic.",
    "Can you help me?",
    "Where is the nearest hospital?",
    "The weather is beautiful today.",
    "Please wait here.",
    "I need glass of water.",
    "Thank you very much!",
    "This book is new.",
    "I am very tired after this long day."
]

# Run translations
for english in examples:
    arabic = translate_english_to_arabic(english, en_ar_model, en_ar_tokenizer)
    print(f"English: {english}")
    print(f"Arabic: {arabic}")
    print("-" * 30)

!pip install gradio
import gradio as gr

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Load AR → EN
ar_en_tokenizer = MarianTokenizer.from_pretrained(ar_en_path)
ar_en_model = MarianMTModel.from_pretrained(ar_en_path).to(device)

# Load EN → AR
en_ar_tokenizer = MarianTokenizer.from_pretrained(en_ar_path)
en_ar_model = MarianMTModel.from_pretrained(en_ar_path).to(device)

# Translation function
def translate(text, direction):
    if direction == "Arabic → English":
        inputs = ar_en_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)
        outputs = ar_en_model.generate(**inputs, max_length=128, num_beams=4)
        return ar_en_tokenizer.decode(outputs[0], skip_special_tokens=True)
    else:
        inputs = en_ar_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)
        outputs = en_ar_model.generate(**inputs, max_length=128, num_beams=4)
        return en_ar_tokenizer.decode(outputs[0], skip_special_tokens=True)

# Gradio UI
iface = gr.Interface(
    fn=translate,
    inputs=[
        gr.Textbox(lines=2, label="Enter sentence"),
        gr.Dropdown(choices=["Arabic → English", "English → Arabic"], value="Arabic → English", label="Direction")
    ],
    outputs="text",
    title="Arabic ↔ English Translator",
    description="Translate between Arabic and English using your fine-tuned MarianMT models. Works both ways!"
)

iface.launch(share=True)

# Reference translations (ground truth in English)
references_ar_en = [
    ["How are you?"],
    ["I love learning English."],
    ["Can you help me?"],
    ["Where is the nearest train station?"],
    ["The weather is nice today."],
    ["What is your name?"],
    ["Please wait here."],
    ["I want a glass of water."],
    ["Thank you very much!"],
    ["This book is new."],
    ["I want to sleep after this long day."],
    ["This project really exhausted me."]
]

# Arabic inputs
arabic_inputs = [
    "كيف حالك؟",
    "أنا أحب تعلم اللغة الإنجليزية.",
    "هل يمكنك مساعدتي؟",
    "أين أقرب محطة قطار؟",
    "الطقس جميل اليوم.",
    "ما اسمك؟",
    "من فضلك، انتظر هنا.",
    "أريد كوباً من الماء.",
    "شكراً جزيلاً لك!",
    "هذا الكتاب جديد.",
    "اريد الذهاب الي النوم بعد هذا اليوم الطويل",
    "لقد ارهقني هذا المشروع فعلا"
]

# Generate model predictions
predictions_ar_en = [
    translate_arabic_to_english(sent, ar_en_model, ar_en_tokenizer)
    for sent in arabic_inputs
]

# Compute BLEU score
references_tokenized = [[nltk.word_tokenize(ref[0])] for ref in references_ar_en]
predictions_tokenized = [nltk.word_tokenize(pred) for pred in predictions_ar_en]

bleu_score_ar_en = corpus_bleu(references_tokenized, predictions_tokenized)
print(f"BLEU score for Arabic → English: {bleu_score_ar_en:.4f}")

# Reference translations (ground truth in Arabic)
references_en_ar = [
    ["كيف حالك؟"],
    ["أنا أحب تعلم اللغة العربية."],
    ["هل يمكنك مساعدتي؟"],
    ["أين أقرب مستشفى؟"],
    ["الطقس جميل اليوم."],
    ["من فضلك، انتظر هنا."],
    ["أريد كوباً من الماء."],
    ["شكراً جزيلاً لك!"],
    ["هذا الكتاب جديد."],
    ["أنا متعب جداً بعد هذا اليوم الطويل."]
]

# English inputs
english_inputs = [
    "How are you?",
    "I love learning Arabic.",
    "Can you help me?",
    "Where is the nearest hospital?",
    "The weather is beautiful today.",
    "Please wait here.",
    "I want a glass of water.",
    "Thank you very much!",
    "This book is new.",
    "I am very tired after this long day."
]

# Generate model predictions
predictions_en_ar = [
    translate_english_to_arabic(sent, en_ar_model, en_ar_tokenizer)
    for sent in english_inputs
]

# Compute BLEU score
references_tokenized = [[nltk.word_tokenize(ref[0])] for ref in references_en_ar]
predictions_tokenized = [nltk.word_tokenize(pred) for pred in predictions_en_ar]

bleu_score_en_ar = corpus_bleu(references_tokenized, predictions_tokenized)
print(f"BLEU score for English → Arabic: {bleu_score_en_ar:.4f}")